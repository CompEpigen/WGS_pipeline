# From a VCF of SVs generated by manta, filter out SVs:
# - whose length is too short
# - which have insufficient Read Pair or Split Reads support
# - (optionally) for which one breakend is in a region present in the Panel of Normal
# - short insertions which likely correspond to retrotransposons (except when they are in the vicinity of a gene). This filtering requires 2 breakpoints.
# (optionally) flags SVs for which one breakend is close to a low mappability region
# (optionally) checks that deletions/duplications are supported by the coverage

import os
import argparse
import numpy as np
import vcfpy
import pysam
from pyensembl import ensembl_grch37

parser = argparse.ArgumentParser()
parser.add_argument('-i', type = str, help='VCF generated by manta')
parser.add_argument('-o', type = str, help='Output VCF')
parser.add_argument('--pon', type = str, help='bed file containing regions to filter out.')
parser.add_argument('--minPR', type = int,default=0, help='Minimum number of paired reads supporting the SV')
parser.add_argument('--minSR', type = int,default=0, help='Minimum number of split reads supporting the SV')
parser.add_argument('--minLen', type = int,default=20000, help='Minimum SV length')
parser.add_argument('--bam', type = str, help='BAM file (used to compute local coverage, to confirm/refute deletions and duplications)')
parser.add_argument('--mappability', type = str, help='bed file containing the regions with low mappability')
parser.add_argument('--tumorindex', type = int,default=0, help='Index of the tumor sample (0 if only the tumor sample was provided, 1 if a control was used).')
parser.add_argument('--keepCloseSV', type = int,default=0, help='If set to 1, will not filter out two SVs whose breakpoints are close to each other (could be reciprocal translocation)')
args = parser.parse_args()


#sample = "C010-AML-15PB8708"
#args.i = "/home/e840r/Documents/WGS/SVs/manta/filtered/"+sample+".vcf"
#args.o = "/home/e840r/Documents/WGS/SVs/manta/filtered_PoN/"+sample+".vcf"
#args.f= "/home/e840r/Documents/WGS/healthy/SV/PoN_SV.bed"

window_size = 3000 # for filtering out small insertions. 


def locus_in_bedRegion(file,chr,pos):
    """
    Parse the bed files of filtered regions to see if one position is present in one of these regions.
    Start from the last position, to avoid iterating several times through the file (the files are assumed to be sorted)
    """
    current_chr = "0"
    start = 0
    end=0
    line = True
    #print("------ Looking for "+chr+"_"+str(pos))
  
    last_pos_file = file.tell()
    pos_file = file.tell()
    while line and current_chr!=chr: # get to the right chromosome
        #print(line)
        last_pos_file = file.tell()
        pos_file = file.tell()
        line = file.readline()
        if line:
            linesplit = line.split("\t")
            current_chr = linesplit[0]
            start = int(linesplit[1])
            end = int(linesplit[2])
  
    while line and current_chr==chr and end<pos:
        last_pos_file = pos_file
        pos_file = file.tell()
        line = file.readline()
        if line:
            linesplit = line.split("\t")
            current_chr = linesplit[0]
            start = int(linesplit[1])
            end = int(linesplit[2])
    file.seek(last_pos_file) # go back one line 

    if pos <= end and pos >= start:
        return True
    else:
        return False



# Get all of the positions of the breakends
positions = set()
reader = vcfpy.Reader.from_path(args.i)
for record in reader:
    chr = record.CHROM
    pos = record.POS
    positions.add((chr,pos))
    if record.INFO["SVTYPE"][:3] in ["DEL","DUP","INS","INV"]:
        chr2 = chr
        pos2 = record.INFO["END"]
    else:
        chr2 = record.ALT[0].mate_chrom
        pos2 = record.ALT[0].mate_pos
    positions.add((chr2,pos2))

def sort_chr_pos(t):
    chr = t[0]
    pos = t[1]
    if chr.isdigit(): chr = int(chr)
    else:
        if chr=="X": chr = 23
        else: chr=24
    return chr * 400000000 + pos

positions = sorted(list(positions), key = sort_chr_pos)

# Go through the PoN file to see which of the breakends are filtered out
pos_is_filtered={}
if args.pon is not None:
    print("PoN was provided")
    file_filter = open(args.pon,"r")
    for pos in positions:
        pos_is_filtered[pos] = locus_in_bedRegion(file_filter,pos[0],pos[1])
    file_filter.close()
else:
    print("PoN was not provided")
    for pos in positions:
        pos_is_filtered[pos] = False


def collect_SVs_close(chr,pos,w):
    records= []
    reader_unfiltered = vcfpy.Reader.from_path(args.i)
    for record in reader_unfiltered:
        if chr == record.CHROM and abs(pos-record.POS) < w and pos!=record.POS:
            records.append(record)
    return records

#############################################
# COVERAGE 
def compute_coverage_region(samfile,chr,start,end,binsize=100000):
    # Divide region into bins to avoid a very high memory usage
    subset_bins = ( end - start > 40*binsize ) # if region is too large, randomly select a subset of bins to estimate the coverage. 
    bins = [start]
    current_pos = start + binsize
    while current_pos < end:
        bins.append(current_pos)
        current_pos+=binsize
    bins.append(end)
    
    bins_index = range(0,len(bins)-2) if not subset_bins else np.random.choice(range(0,len(bins)-2),20,replace=False)
    means = []
    for i in bins_index:
        try:
            c = samfile.count_coverage(contig=chr,start=bins[i],stop=bins[i]+binsize)
            c = np.sum([np.mean(x) for x in c])
            if c>2:
                means.append(c)
        except:
            pass
    sum_coverage = np.sum(means)

    # last bin might have a different size
    if not subset_bins:
        try:
            c = samfile.count_coverage(contig=chr,start=bins[-2],stop=bins[-1])
            c = np.sum([np.mean(x) for x in c])
            if c>2:
                sum_coverage+= (bins[-1]-bins[-2]) / binsize * c
        except:
            pass
    if subset_bins:
        avg_coverage = sum_coverage / len(bins_index)
    else:
        avg_coverage = sum_coverage / ( len(bins_index) + (bins[-1]-bins[-2]) / binsize )
    return avg_coverage

def estimate_global_coverage(samfile):
    """ Selects some random regions instead of actually doing it on the whole genome. Avoid regions with 0 coverage."""
    means = []
    for chr in range(1,23):
        chr = str(chr)
        for pos in np.random.randint(1,2*10**8,5):
            try:
                cov = compute_coverage_region(samfile,chr,pos,pos+50000)
                if cov>2:
                    means.append(cov)
            except:
                pass
    return np.mean(means)

global_coverage=-1
if args.bam is not None:
    samfile = pysam.AlignmentFile(args.bam, "rb")
    global_coverage = estimate_global_coverage(samfile)
    print("Global coverage: " + str(global_coverage))
else:
    print("BAM file was not provided")
# END COVERAGE
####################################################


def get_breakpoint_info(record):
    chr = record.CHROM
    pos = record.POS
    if record.INFO["SVTYPE"] in ["DEL","DUP","INS"]:
        chr2 = chr
        pos2 = record.INFO["END"]
        if record.INFO["SVTYPE"]=="DEL":
            orientation = "-"
            orientation2 = "+"
        elif record.INFO["SVTYPE"]=="DUP":
            orientation = "+"
            orientation2 = "-"
        else:
            orientation = "-"
            orientation2 = "+"
    else:
        chr2 = record.ALT[0].mate_chrom
        pos2 = record.ALT[0].mate_pos
        orientation = record.ALT[0].orientation
        orientation2 = record.ALT[0].mate_orientation
    return ( (chr,pos,orientation) , (chr2,pos2,orientation2) )

def region_contains_genes(chr,start,end):
    return len(ensembl_grch37.genes_at_locus(chr,start,end))>0

def low_mapq_in_region(samfile,chr,start,end,mapq_threshold=45):
    count_lowmapq=0
    count_highmapq=0
    for read in samfile.fetch(chr,start,end):
        if read.mapq<mapq_threshold:
            count_lowmapq+=1
        else:
            count_highmapq+=1
    return count_lowmapq/(count_lowmapq+count_highmapq) >=0.15

def SV_explained_by_alternative_alignments(samfile,chr1,pos1,chr2,pos2):
    # Find reads supporting the alignment
    count_reads_supportingSV=0
    count_reads_supportingSV_withXA=0
    for read in samfile.fetch(chr1, pos1-200, pos2+200):
        # Check for split-read (supplementary alignment)
        SA_chr=""
        for t in read.tags:
            if t[0]=="SA":
                for s in t[1][:-1].split(";"):
                    s_split = s.split(",")
                    s_chr = s_split[0]
                    s_start = int(s_split[1])
                    if (s_chr==chr2 and abs(pos2-s_start)<200):
                        SA_chr=s_chr
                        SA_start = s_start
        # Check for read pair supporting the SV
        read_pair_SV = read.next_reference_name==chr2 and abs(read.next_reference_start-pos2)<200

        if SA_chr!="" or read_pair_SV: # The read supports the SV
            count_reads_supportingSV+=1
            for t in read.tags:
                if t[0]=="XA":
                    count_reads_supportingSV_withXA+=1

    print("Reads supporting SV: " +str(count_reads_supportingSV)+"; with XA: "+str(count_reads_supportingSV_withXA))
    return False

def reads_go_through_insertion(samfile,chr1,pos1,chr2,pos2):
    """Look for read pairs which go through an insertion"""
    count_insertions=0
    for read in samfile.fetch(chr1,pos1-400,pos1+400):
        if readpair_goes_through_insertion(read,chr1,pos1,chr2,pos2,1000):
            count_insertions+=1
    if count_insertions>0:
        print("Insertion count: "+ str(count_insertions))
    return count_insertions>0


def readpair_goes_through_insertion(read,chr1,pos1,chr2,pos2,window_size=1000):
    """Look for a read pair where both main alignments are in the same region, but there is a supplementary alignment in between which maps to the other region."""
    has_SA=False
    for t in read.tags:
        if t[0]=="SA":
            for s in t[1][:-1].split(";"):
                s_split = s.split(",")
                s_chr = s_split[0]
                s_start = int(s_split[1])
                if (s_chr==chr1 and abs(pos1-s_start)<window_size) or (s_chr==chr2 and abs(s_start-pos2) < window_size):
                    SA_chr=s_chr
                    SA_start = s_start
                    has_SA=True
    if not has_SA: return False

    # Check that both read pairs map to the same region
    if read.next_reference_name != read.reference_name or abs(read.next_reference_start-read.reference_start)>window_size: return False

    # Check that the supplementary alignment is between the main alignment and the mate
    if read.cigarstring.find("S")>0: charS="S"
    if read.cigarstring.find("H")>0: charS="H"
    if read.is_reverse:
        sup_alignment_middle = read.cigarstring.find(charS) < read.cigarstring.find("M")
    else:
        sup_alignment_middle = read.cigarstring.find(charS) > read.cigarstring.find("M")
    if not sup_alignment_middle: return False

    # Check that the main alignment maps to one of the 2 regions and that the supplementary alignment maps to the other region
    if read.reference_name==chr1 and abs(read.reference_start-pos1)<window_size:
        if SA_chr==chr2 and abs(SA_start-pos2)<window_size:
            return True
    elif read.reference_name==chr2 and abs(read.reference_start-pos2)<window_size:
        if SA_chr==chr1 and abs(SA_start-pos1)<window_size:
            return True

    return False

#########################################################

    

# Write the filtered VCF
reader = vcfpy.Reader.from_path(args.i)
header = reader.header
#header.add_filter_line({"ID":"MAPPABILITY","Description":"Regions with low mappability close to the breakpoint."})
writer = vcfpy.Writer.from_path(args.o,reader.header)
for record in reader:
    print("-----------")
    print(record)
    ( (chr,pos,orientation) , (chr2,pos2,orientation2) ) = get_breakpoint_info(record)

   
    # Filter based on minimum number of supporting reads
    if (not "PR" in record.calls[args.tumorindex].data) or (not "SR" in record.calls[args.tumorindex].data):
        filtered_n_reads=True
    else:
        filtered_n_reads = (record.calls[args.tumorindex].data["PR"][1] < args.minPR) or (record.calls[args.tumorindex].data["SR"][1] < args.minSR)
        # Somatic SVs (and heterozygous): expect some reads which do not support the SV.
        filtered_n_reads = filtered_n_reads or (record.calls[args.tumorindex].data["PR"][0] < 2) or (record.calls[args.tumorindex].data["SR"][1] < 2)
    if filtered_n_reads:
        print("SV filtered due to insufficient number of supporting reads")
        continue

    # Filter based on minimum SV length
    filtered_length = (chr==chr2) and ( abs(pos-pos2) < args.minLen)
    if filtered_length:
        print("SV filtered due to short length")
        continue
    # Filter based on Panel of Normal
    one_breakend_filtered = pos_is_filtered[(chr,pos)] or pos_is_filtered[(chr2,pos2)]
    if one_breakend_filtered:
        print("SV filtered by Panel of Normal")
        continue

    # Filter out SVs where many of the reads in the region have low MAPQ, since these regions are unreliable.
    if low_mapq_in_region(samfile,chr,pos-100,pos+100,45) or low_mapq_in_region(samfile,chr2,pos2-100,pos2+100,45):
        print("Filtered out because many of the reads near the breakpoint had a low MAPQ.")
        continue

    # Filter out reads with PolyA insertions, since they probably come from retrotransposons
    if "sequence" in dir(record.ALT[0]) and len(record.ALT[0].sequence) > 8 and (record.ALT[0].sequence =="A"*len(record.ALT[0].sequence) or record.ALT[0].sequence =="T" * len(record.ALT[0].sequence)):
        print("PolyA insertion: probably retrotransposon --> filter out")
        continue 

    # Check if deletions and duplications are validated by read depth, in which case we accept them directly. 
    """
    if record.INFO["SVTYPE"] == "DEL" and global_coverage>=0:
        coverage_del = compute_coverage_region(samfile,chr,pos,pos2)
        if coverage_del < 0.75 * global_coverage:
            print("Deletion confirmed based on read depth")
            writer.write_record(record)
            continue
        else:
            print("Deletion not confirmed based on read depth") 
            # Consider as BND instead of deletion...
            record.ALT[0] = vcfpy.BreakEnd(mate_chrom=chr,mate_pos = pos2, orientation = "-", mate_orientation="+",sequence="",within_main_assembly=True)
    elif record.INFO["SVTYPE"] == "DUP" and global_coverage>=0:
        coverage_dup = compute_coverage_region(samfile,chr,pos,pos2)
        if coverage_dup > 1.25 * global_coverage:
            print("Duplication confirmed based on read depth")
            writer.write_record(record)
            continue
        else:
            print("Duplication not confirmed based on read depth") 
            # Consider as BND instead of duplication...
            record.ALT[0] = vcfpy.BreakEnd(mate_chrom=chr,mate_pos = pos2, orientation = orientation, mate_orientation=orientation2,sequence="",within_main_assembly=True)
    """

    if region_contains_genes(chr,pos-10000,pos+10000) or region_contains_genes(chr,pos2-10000,pos2+10000):
        print("Breakpoint close to gene")
        #writer.write_record(record)
        #continue

    # See if a second SV can, combined with the first one, lead to a small insertion, which would then be filtered out.
    filter_out_record = False
    for r in collect_SVs_close(chr,pos,window_size):
        print(r)
        ( (Bchr,Bpos,Borientation) , (Bchr2,Bpos2,Borientation2) ) = get_breakpoint_info(r)
        if Bchr2 ==chr2 and abs(pos2-Bpos2) < window_size:
            # WARNING: Can have reciprocal translocation with small duplication, which would then look like a small insertion... 
            # In principle, we could use the full read information with the BAM, to check... 
            if pos < Bpos:
                orientationOutward = (orientation=="+")
                BorientationOutward = (Borientation=="-")
            else:
                orientationOutward = (orientation=="-")
                BorientationOutward = (Borientation=="+")
            if pos2<Bpos2:
                orientationOutward2 = (orientation2=="+")
                BorientationOutward2 = (Borientation2=="-")
            else:
                orientationOutward2 = (orientation2=="-")
                BorientationOutward2 = (Borientation2=="+")
            
            if abs(pos-Bpos)<100 and abs(pos2-Bpos2)<100: 
                if reads_go_through_insertion(samfile,chr,pos,chr2,pos2):
                    print("Small insertion, because some read pairs going through the whole insertion were detected")
                    filter_out_record=True
            elif abs(pos-Bpos)<1000 and abs(pos2-Bpos2)<1000 and args.keepCloseSV:
                print("Close SVs") 
            elif (not orientationOutward) and (not BorientationOutward) and (not orientationOutward2) and (not BorientationOutward2):
                if chr ==chr2:
                    print("Inversion") # TODO: inverted duplication ??
                else:
                    print("Reciprocal translocation")
            elif orientationOutward and BorientationOutward and orientationOutward2 and BorientationOutward2:
                # Insertion with TSD
                print("Small insertion with TSD -> filtered out")
                filter_out_record = True
                continue
            elif ( orientationOutward and BorientationOutward and (not orientationOutward2) and not (BorientationOutward2) ) \
                or ( (not orientationOutward) and (not BorientationOutward) and orientationOutward2 and BorientationOutward2 ):
                # Insertion from the first side into the second side, with a deletion at the insertion site.
                print("Small insertion, with a small deletion at the inserted site -> filtered out")
                filter_out_record = True
                continue
            else:
                print("Rearrangement is not clear -> keep.")
        print("-")

    if filter_out_record: continue
    
    print("Keep SV.")
    writer.write_record(record)
